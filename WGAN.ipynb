{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN86aqgiDwTUOEl/f9co4We",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarveshD7/WGAN-Pytorch/blob/main/WGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Pros of WGAN***\n",
        "Better Stability\n",
        "\n",
        "Loss means something unlike in normal GANs.\n",
        "Loss: Terminating criteria\n",
        "\n",
        "Prevernts Mode Collapse:\n",
        "# Mode collapse\n",
        "Is when the model outputs only a specific class images\n",
        "Usually you want your GAN to produce a wide variety of outputs. You want, for example, a different face for every random input to your face generator.\n",
        "\n",
        "However, if a generator produces an especially plausible output, the generator may learn to produce only that output. In fact, the generator is always trying to find the one output that seems most plausible to the discriminator.\n",
        "\n",
        "If the generator starts producing the same output (or a small set of outputs) over and over again, the discriminator's best strategy is to learn to always reject that output. But if the next generation of discriminator gets stuck in a local minimum and doesn't find the best strategy, then it's too easy for the next generator iteration to find the most plausible output for the current discriminator.\n",
        "\n",
        "Each iteration of generator over-optimizes for a particular discriminator, and the discriminator never manages to learn its way out of the trap. As a result the generators rotate through a small set of output types. This form of GAN failure is called mode collapse.\n",
        "\n",
        "# Wasserstein loss:\n",
        "The Wasserstein loss alleviates mode collapse by letting you train the discriminator to optimality without worrying about vanishing gradients. If the discriminator doesn't get stuck in local minima, it learns to reject the outputs that the generator stabilizes on. So the generator has to try something new.\n",
        "\n",
        "# ***Cons of WGAN***\n",
        "Longer to train\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "07tQ8NL_KKmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "WqSt2-5wLSzk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LEARNING_RATE = 5e-5\n",
        "BATCH_SIZE = 64\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 1\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS = 5\n",
        "FEATURES_critic = 64\n",
        "FEATURES_GEN = 64\n",
        "CRITIC_ITERATIONS = 5\n",
        "WEIGHT_CLIP = 0.01"
      ],
      "metadata": {
        "id": "8WmdLVA3UXqE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, channels_img, features_d):  # features_d is channels that are going to change in different layers\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.critic = nn.Sequential(\n",
        "        # Input: N x channels_img x 64 x 64\n",
        "        nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),  # 32x32\n",
        "        nn.LeakyReLU(0.2),\n",
        "        self._block(features_d, features_d*2, 4,2,1),  # 16x16\n",
        "        self._block(features_d*2, features_d*4, 4,2,1),  # 8x8\n",
        "        self._block(features_d*4, features_d*8, 4,2,1),  # 4x4\n",
        "        nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0), # 1x\n",
        "    )\n",
        "\n",
        "  def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        # nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(0.2)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.critic(x)\n"
      ],
      "metadata": {
        "id": "YpA6WsGlUv4H"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, z_dim, channels_img, features_g):\n",
        "    super(Generator, self).__init__()\n",
        "    # Input: N x z_dim x 1 x 1\n",
        "    self.gen = nn.Sequential(\n",
        "        self._block(z_dim, features_g*16, 4, 1, 0), # 4x4\n",
        "        self._block(features_g*16, features_g*8, 4, 2, 1), # 8x8\n",
        "        self._block(features_g*8, features_g*4, 4, 2, 1), # 16x16\n",
        "        self._block(features_g*4, features_g*2, 4, 2, 1), # 32x32\n",
        "        nn.ConvTranspose2d(features_g*2,channels_img, kernel_size=4, stride=2, padding=1),  # 64x64\n",
        "        nn.Tanh()  # Range = [-1,1]\n",
        "    )\n",
        "\n",
        "  def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        # nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.gen(x)\n"
      ],
      "metadata": {
        "id": "aa-8UIIBU2FW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing weights of the model with a normal distribution with given mean and standard deviation\n",
        "def initialize_weights(model):\n",
        "  for m in model.modules():\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "      nn.init.normal_(m.weight.data, 0.0, 0.02)"
      ],
      "metadata": {
        "id": "DKAIKUhCU6qc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = torchvision.transforms.Compose(\n",
        "   [ torchvision.transforms.Resize(IMAGE_SIZE),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(\n",
        "        [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)],\n",
        "    )]\n",
        ")"
      ],
      "metadata": {
        "id": "iNlgnuxFU-Fa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.MNIST(root=\"/content/dataset/\", train=True, transform=transforms, download=True)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "tKSsM2hNVmZW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
        "critic = Discriminator(CHANNELS_IMG, FEATURES_critic).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)"
      ],
      "metadata": {
        "id": "QQwp8CWyWp_8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_gen = optim.RMSprop(gen.parameters(), lr=LEARNING_RATE)\n",
        "opt_critic = optim.RMSprop(critic.parameters(), lr=LEARNING_RATE)\n",
        "# criterion = nn.BCELoss()  Not needed in WGAN\n"
      ],
      "metadata": {
        "id": "6CfdtUnoVpSW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen.train()\n",
        "critic.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jho-nPdWi6D",
        "outputId": "c3abeafb-7848-41da-e289-5d634e38bbb3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (disc): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2)\n",
              "    (2): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
              "    (6): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrting the training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(f\"Started epoch-> {epoch}\")\n",
        "  for batch_idx, (real, _) in enumerate(loader):\n",
        "    real = real.to(device)\n",
        "    # Train the Discriminator (Critic) CRITIC_ITERATIONS time for every time Generator is trained\n",
        "    for _ in range(CRITIC_ITERATIONS):\n",
        "      noise = torch.randn((BATCH_SIZE, Z_DIM, 1, 1)).to(device)\n",
        "      fake = gen(noise)\n",
        "      critic_real = critic(real).reshape(-1)\n",
        "      critic_fake = critic(fake).reshape(-1)\n",
        "      loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
        "      critic.zero_grad()\n",
        "      loss_critic.backward(retain_graph=True)  # retain_graph=True since we want to reuse fake and Pytorch erases intermediate results on loss.backward()\n",
        "      opt_critic.step()\n",
        "\n",
        "      for p in critic.parameters():\n",
        "        p.data.clamp_(WEIGHT_CLIP, WEIGHT_CLIP)\n",
        "    # Training the Generator\n",
        "    output = critic(fake).reshape(-1)\n",
        "    loss_gen = -torch.mean(output)\n",
        "    gen.zero_grad()\n",
        "    loss_gen.backward()\n",
        "    opt_gen.step()\n",
        "  print(f\"Completed epoch-> {epoch}\")\n"
      ],
      "metadata": {
        "id": "xBFwMQO5XL7y"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}